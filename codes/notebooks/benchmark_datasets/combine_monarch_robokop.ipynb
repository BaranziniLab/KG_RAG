{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "b26f08be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "from neo4j import GraphDatabase, basic_auth\n",
    "import re\n",
    "from utility import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "fb4149ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mcq_with_shuffle(df, source_column, target_column, node_type, predicate):\n",
    "    disease_pairs = df[source_column].unique()\n",
    "    disease_pairs = [(disease1, disease2) for disease1 in disease_pairs for disease2 in disease_pairs if disease1 != disease2]\n",
    "\n",
    "    new_data = []\n",
    "\n",
    "    #For each source pair, find a common target and 4 negative samples\n",
    "    for disease1, disease2 in disease_pairs:\n",
    "        common_gene = set(df[df[source_column] == disease1][target_column]).intersection(set(df[df[source_column] == disease2][target_column]))\n",
    "        common_gene = list(common_gene)[0] if common_gene else None\n",
    "        # Get 4 random negative samples\n",
    "        negative_samples = df[(df[source_column] != disease1) & (df[source_column] != disease2)][target_column].sample(4).tolist()\n",
    "        new_data.append(((disease1, disease2), common_gene, negative_samples))\n",
    "\n",
    "    new_df = pd.DataFrame(new_data, columns=[\"disease_pair\", \"correct_node\", \"negative_samples\"])\n",
    "    new_df.dropna(subset = [\"correct_node\"], inplace=True)\n",
    "    new_df.loc[:, \"disease_1\"] = new_df[\"disease_pair\"].apply(lambda x: x[0])\n",
    "    new_df.loc[:, \"disease_2\"] = new_df[\"disease_pair\"].apply(lambda x: x[1])\n",
    "    new_df.negative_samples = new_df.negative_samples.apply(lambda x:\", \".join(x[0:4]))\n",
    "    new_df.loc[:, \"options_combined\"] = new_df.negative_samples.apply(lambda x:x.split(\",\")) + new_df.correct_node.apply(lambda x:x.split(\",\"))\n",
    "    new_df.loc[:, \"options_combined\"] = new_df.options_combined.apply(shuffle_list)\n",
    "    new_df.loc[:, \"options_combined\"] = new_df.options_combined.apply(lambda x:\", \".join(x))\n",
    "    new_df.loc[:, \"text\"] = \"Out of the given list, which \" + node_type + \" \" + predicate + \" \" + new_df.disease_1 + \" and \" + new_df.disease_2 + \". Given list is: \" + new_df.options_combined\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e65657a",
   "metadata": {},
   "source": [
    "# Only include entities that is represented in SPOKE \n",
    "#### Note: \n",
    "#### If the data has '*A-ASSOCIATES-B*', we are making sure A and B are present in SPOKE, and NOT checking if '*A-ASSOCIATES-B*' is present in SPOKE. \n",
    "#### Otherwise, the comaprison, with and without SPOKE, will not be fair (because SPOKE doesn't represent those entities, if it doesn't have them)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "17f68561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.31 s, sys: 939 ms, total: 7.25 s\n",
      "Wall time: 55.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "load_dotenv(os.path.join(os.path.expanduser('~'), '.neo4j_config.env'))\n",
    "USER = os.environ.get('SPOKE_USER')\n",
    "PSW = os.environ.get('SPOKE_PSW')\n",
    "URI = os.environ.get('SPOKE_URI')\n",
    "\n",
    "GENE_QUERY = \"\"\"\n",
    "    MATCH(d:Disease)-[r:ASSOCIATES_DaG]->(g:Gene) \n",
    "    RETURN DISTINCT g.name AS g_name     \n",
    "\"\"\"\n",
    "\n",
    "VARIANT_QUERY = \"\"\"\n",
    "    MATCH(d:Disease)<-[r:ASSOCIATES_VaP]-(v:Variant) \n",
    "    RETURN DISTINCT v.identifier AS v_id     \n",
    "\"\"\"\n",
    "\n",
    "ORGANISM_QUERY = \"\"\"\n",
    "    MATCH(d:Disease)<-[r:CAUSES_OcD]-(o:Organism) \n",
    "    RETURN DISTINCT o.identifier AS o_id     \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "auth = basic_auth(USER, PSW)\n",
    "sdb = GraphDatabase.driver(URI, auth=auth)\n",
    "\n",
    "gene_list = []\n",
    "with sdb.session() as session:\n",
    "    with session.begin_transaction() as tx:\n",
    "        result = tx.run(GENE_QUERY)\n",
    "        for row in result:\n",
    "            gene_list.append(row[\"g_name\"])\n",
    "            \n",
    "variant_list = []\n",
    "with sdb.session() as session:\n",
    "    with session.begin_transaction() as tx:\n",
    "        result = tx.run(VARIANT_QUERY)\n",
    "        for row in result:\n",
    "            variant_list.append(row[\"v_id\"])\n",
    "            \n",
    "\n",
    "organism_list = []\n",
    "with sdb.session() as session:\n",
    "    with session.begin_transaction() as tx:\n",
    "        result = tx.run(ORGANISM_QUERY)\n",
    "        for row in result:\n",
    "            organism_list.append(row[\"o_id\"])\n",
    "            \n",
    "            \n",
    "sdb.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c57cfca",
   "metadata": {},
   "source": [
    "# MONARCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "480c699c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_api_resp(URI, params=None):\n",
    "    if params:\n",
    "        return requests.get(URI, params=params)\n",
    "    else:\n",
    "        return requests.get(URI)\n",
    "    \n",
    "def get_association(URI, disease_id, params, object_attribute=\"label\"):\n",
    "    URI_ = URI.format(disease_id)\n",
    "    resp = get_api_resp(URI_, params=params)\n",
    "    if resp.status_code == 200:\n",
    "        response = resp.json()\n",
    "        associations = response[\"associations\"]\n",
    "        object_list = []\n",
    "        for item in associations:\n",
    "            object_list.append(item[\"object\"][object_attribute])\n",
    "        df = pd.DataFrame(object_list, columns=[\"object\"])\n",
    "        df[\"subject\"] = disease_id\n",
    "        return df\n",
    "    else:\n",
    "        return None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "48b770cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "DISEASE_GENE_URI = \"https://api.monarchinitiative.org/api/bioentity/disease/{}/genes\"\n",
    "DISEASE_VARIANT_URI = \"https://api.monarchinitiative.org/api/bioentity/disease/{}/variants\"\n",
    "disease_path = \"../../../data/benchmark_datasets/monarch/gwas_diseases.csv\"\n",
    "\n",
    "# For extracting Monarch data, we are considering a subset of diseases from SPOKE that has connections with genes from GWAS. These disease are obtained by running CYPHER in neo4j browser and saved as csv file which is loaded below\n",
    "disease_df = pd.read_csv(disease_path)\n",
    "disease_df.columns = [\"disease_id\", \"disease_name\"]\n",
    "disease_df.disease_id = disease_df.disease_id.apply(lambda x:x.split('\"')[1])\n",
    "\n",
    "\n",
    "params = {}\n",
    "params[\"rows\"] = 2\n",
    "params[\"direct\"] = \"true\"\n",
    "params[\"direct_taxon\"] = \"true\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd864ff",
   "metadata": {},
   "source": [
    "## Disease-Gene "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "c7e07861",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "254it [01:46,  2.39it/s]\n"
     ]
    }
   ],
   "source": [
    "edge_df_list = []\n",
    "\n",
    "for index, row in tqdm(disease_df.iterrows()):\n",
    "    edge_df_list.append(get_association(DISEASE_GENE_URI, row[\"disease_id\"], params))\n",
    "\n",
    "edge_df = pd.concat(edge_df_list, ignore_index=True)\n",
    "edge_df = pd.merge(edge_df, disease_df, left_on=\"subject\", right_on=\"disease_id\").drop([\"subject\", \"disease_id\"], axis=1)\n",
    "edge_df.disease_name = edge_df.disease_name.apply(lambda x:x.split('\"')[1])\n",
    "edge_df = edge_df[edge_df.object.isin(gene_list)]\n",
    "monarch_disease_gene_mcq = create_mcq_with_shuffle(edge_df, \"disease_name\", \"object\", \"Gene\", \"is associated with\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63da381f",
   "metadata": {},
   "source": [
    "## Disease-Variant "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "75687a0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "254it [01:44,  2.43it/s]\n"
     ]
    }
   ],
   "source": [
    "edge_df_list_2 = []\n",
    "\n",
    "for index, row in tqdm(disease_df.iterrows()):\n",
    "    edge_df_list_2.append(get_association(DISEASE_VARIANT_URI, row[\"disease_id\"], params, object_attribute=\"id\"))\n",
    "\n",
    "edge_df_2 = pd.concat(edge_df_list_2, ignore_index=True)\n",
    "edge_df_2 = pd.merge(edge_df_2, disease_df, left_on=\"subject\", right_on=\"disease_id\").drop([\"subject\", \"disease_id\"], axis=1)\n",
    "edge_df_2.disease_name = edge_df_2.disease_name.apply(lambda x:x.split('\"')[1])\n",
    "edge_df_2.object = edge_df_2.object.apply(lambda x:x.split(\"dbSNP:\")[-1])\n",
    "edge_df_2 = edge_df_2[edge_df_2.object.isin(variant_list)]\n",
    "\n",
    "monarch_disease_variant_mcq = create_mcq_with_shuffle(edge_df_2, \"disease_name\", \"object\", \"Variant\", \"is associated with\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbc068d",
   "metadata": {},
   "source": [
    "# ROBOKOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "d51f3502",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROBOKOP_PATH = \"../../../data/benchmark_datasets/robokop\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce0ce8c",
   "metadata": {},
   "source": [
    "## Disease-Variant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "b90bb109",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILES_LIST_2 = [\"disease_variant_1.csv\", \"disease_variant_2.csv\"]\n",
    "\n",
    "data_robokop = []\n",
    "for item in FILES_LIST_2:\n",
    "    data_robokop.append(pd.read_csv(os.path.join(ROBOKOP_PATH, item)))\n",
    "    \n",
    "data_robokop = pd.concat(data_robokop, ignore_index=True)\n",
    "data_robokop.columns = [\"source\", \"target\"]\n",
    "data_robokop = data_robokop[data_robokop.target.isin(variant_list)]\n",
    "robokop_disease_variant_mcq = create_mcq_with_shuffle(data_robokop, \"source\", \"target\", \"Variant\", \"is associated with\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d71955",
   "metadata": {},
   "source": [
    "## Disease-Organism "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "6dbc39cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILES_LIST_3 = [\"disease_organism_1.csv\"]\n",
    "\n",
    "data_robokop = []\n",
    "for item in FILES_LIST_3:\n",
    "    data_robokop.append(pd.read_csv(os.path.join(ROBOKOP_PATH, item)))\n",
    "    \n",
    "data_robokop = pd.concat(data_robokop, ignore_index=True)\n",
    "\n",
    "def extract_doid(entry):\n",
    "    matches = re.findall(r'DOID:\\d+', entry)\n",
    "    return matches\n",
    "\n",
    "data_robokop['extracted_DOIDs'] = data_robokop['d.equivalent_identifiers'].apply(extract_doid)\n",
    "data_robokop = data_robokop.explode(\"extracted_DOIDs\").dropna(subset=[\"extracted_DOIDs\"]).drop(\"d.equivalent_identifiers\", axis=1)\n",
    "data_robokop.columns = [\"source\", \"target\", \"target_id\", \"source_id\"]\n",
    "data_robokop.loc[:, \"target_id\"] = data_robokop.target_id.apply(lambda x:x.split(\"NCBITaxon:\")[-1])\n",
    "data_robokop.target_id = data_robokop.target_id.astype(int)\n",
    "\n",
    "# Include only those entities that are present in SPOKE (Note: only entities, not the association)\n",
    "data_robokop = data_robokop[data_robokop.target_id.isin(organism_list)]\n",
    "\n",
    "robokop_disease_organism_mcq = create_mcq_with_shuffle(data_robokop, \"source\", \"target\", \"Organism\", \"causes\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "397d0145",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_combined = pd.concat([monarch_disease_gene_mcq, monarch_disease_variant_mcq, robokop_disease_variant_mcq, robokop_disease_organism_mcq], ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "810e7ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_combined.to_csv(\"../../../data/benchmark_datasets/test_questions_two_hop_mcq_from_monarch_and_robokop.csv\", index=False, header=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "5f32fae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out of the given list, which Gene is associated with psoriasis and allergic rhinitis. Given list is:  ATP2B1, HLA-B,  STAT4,  TERT, FADS1\n",
      "HLA-B\n"
     ]
    }
   ],
   "source": [
    "print(data_combined.text.values[3])\n",
    "print(data_combined.correct_node.values[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7af708",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
